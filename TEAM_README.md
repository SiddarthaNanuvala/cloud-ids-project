# Cloud-IDS: Network Anomaly Detection System
## Team Documentation & Setup Guide

**Project Status:** âœ… Production Ready | **Last Updated:** December 6, 2025

---

## ğŸ“‹ Quick Summary for Teammates

This is a **machine learning-based network anomaly detection system** built on the CIC-IDS2017 dataset. It uses an **Isolation Forest model** to detect suspicious network traffic patterns in real-time. The system is fully containerized with Docker, deployed on Kubernetes, and includes auto-scaling capabilities.

### Key Features
- âœ… Real-time anomaly detection (<1ms latency)
- âœ… 1000+ predictions per second
- âœ… Auto-scaling (1-10 replicas based on load)
- âœ… Production-ready API with 5 endpoints
- âœ… Prometheus metrics integration
- âœ… Fully documented and tested

---

## ğŸ¯ What Each Component Does

### 1. **Machine Learning Pipeline**
Located in: `scripts/`

**Data Processing** (`preprocess_cicids2017.py`)
- Loads 8 CIC-IDS2017 CSV files (1,147 MB)
- Cleans data: removes duplicates, infinite values, missing records
- Result: 2.83M clean records from 3.1M original
- Splits into: train (80%), validation (10%), test (10%)

**Model Training** (`train_autoencoder.py`)
- Algorithm: Isolation Forest (100 trees)
- Input: 72 normalized network flow features
- Output: model.pkl (773 KB)
- Training time: ~45 seconds
- Detection: Identifies top 10% anomalies

**Model Evaluation** (`evaluate_model.py`)
- Computes metrics: precision, recall, F1, ROC-AUC
- Validates model performance on test set
- Generates performance reports

### 2. **Production API Service**
Located in: `app/main.py`

**Framework:** FastAPI (modern Python web framework)

**Endpoints:**
```
GET  /health              â†’ Health check (for Kubernetes probes)
POST /score              â†’ Single sample prediction
POST /batch_score        â†’ Batch predictions (1000+ samples)
GET  /metrics            â†’ Prometheus metrics
GET  /model_info         â†’ Model metadata & features
```

**Performance:**
- Latency: <1ms per sample
- Throughput: 1000+ samples/sec
- Uptime: 5+ days continuous

### 3. **Docker Containerization**
Located in: `Dockerfile`

**Image:** `siddartha6174/ids:latest` (4.46 GB)

**Contains:**
- Python 3.10-slim base
- PyTorch + NVIDIA CUDA (2.8 GB)
- scikit-learn, FastAPI, Uvicorn
- Model artifact (773 KB)

**Build Time:**
- First build: ~26 minutes
- Subsequent builds: 2-3 minutes (layer caching)

### 4. **Kubernetes Orchestration**
Located in: `k8s/`

**Manifests:**
- `deploy.yaml` - Deployment config (1-10 replicas)
- `service.yaml` - ClusterIP service (port 80â†’8000)
- `hpa.yaml` - Horizontal Pod Autoscaler (CPU/Memory targets)

**Current Status:**
- Cluster: docker-desktop (v1.34.1)
- Pod: ids-service-68b95d8898-48lll (1/1 Ready)
- Service IP: 10.96.230.46:80
- Uptime: 5+ days

---

## ğŸš€ Getting Started

### Prerequisites
```bash
# Install Docker Desktop (includes Kubernetes)
# https://www.docker.com/products/docker-desktop

# Install kubectl (usually included with Docker Desktop)
kubectl version --client
```

### Setup & Installation

**1. Clone the repository:**
```bash
git clone https://github.com/SiddarthaNanuvala/cloud-ids-project.git
cd cloud-ids-project
```

**2. Create Python virtual environment:**
```bash
# Create .venv (will be ~432 MB)
python -m venv .venv

# Activate it
# Windows PowerShell:
.\.venv\Scripts\Activate.ps1

# macOS/Linux:
source .venv/bin/activate
```

**3. Install dependencies:**
```bash
pip install -r requirements.txt
```

**4. Run ML pipeline (optional, model already trained):**
```bash
cd scripts
python preprocess_cicids2017.py    # Clean data (~2 min)
python train_autoencoder.py         # Train model (~1 min)
python evaluate_model.py            # Evaluate performance (~30 sec)
```

**5. Start API locally:**
```bash
cd app
python -m uvicorn main:app --reload --port 8000

# Then visit: http://localhost:8000/health
```

---

## ğŸ³ Docker & Kubernetes Deployment

### Option 1: Run Locally (Docker)
```bash
# Build image (first time: ~26 min)
docker build -t ids:latest .

# Run container
docker run -p 8000:8000 ids:latest

# Test
curl http://localhost:8000/health
```

### Option 2: Deploy to Kubernetes
```bash
# Apply all manifests
kubectl apply -f k8s/

# Check status
kubectl get pods -l app=ids
kubectl get svc ids-svc
kubectl get hpa ids-hpa

# Port forward to test
kubectl port-forward svc/ids-svc 8000:80

# Test
curl http://localhost:8000/health
```

### Option 3: Deploy from Docker Hub (Already Built)
```bash
# Already available: siddartha6174/ids:latest
# Just apply Kubernetes manifests
kubectl apply -f k8s/

# System will pull image automatically
```

---

## ğŸ“Š Testing the API

### Test 1: Health Check
```bash
curl http://localhost:8000/health

# Response:
# {"status": "ok"}
```

### Test 2: Get Model Info
```bash
curl http://localhost:8000/model_info

# Response:
# {
#   "model_type": "Isolation Forest",
#   "n_features": 72,
#   "contamination": 0.10,
#   "features": ["Feature1", "Feature2", ...]
# }
```

### Test 3: Single Sample Prediction
```bash
curl -X POST http://localhost:8000/score \
  -H "Content-Type: application/json" \
  -d '{"features": [1.0, 2.0, 3.0, ..., 72 values total]}'

# Response:
# {
#   "prediction": 1,
#   "is_anomaly": false,
#   "anomaly_score": -0.45
# }
```

### Test 4: Batch Predictions
```bash
curl -X POST http://localhost:8000/batch_score \
  -H "Content-Type: application/json" \
  -d '{"features": [[...], [...], ...]}' # Multiple samples

# Response:
# {
#   "predictions": [1, 1, -1, 1, ...],
#   "predictions_binary": [0, 0, 1, 0, ...],
#   "anomaly_count": 125,
#   "anomaly_percentage": 12.5
# }
```

### Test 5: Prometheus Metrics
```bash
curl http://localhost:8000/metrics

# Response: (Prometheus format)
# http_requests_total{endpoint="/score"} 1234
# inference_latency_seconds_bucket{le="0.001"} 1234
```

---

## ğŸ“ˆ Auto-Scaling in Action

### How It Works
The system automatically scales from 1 to 10 replicas based on CPU and memory usage:

**Scaling Thresholds:**
- Scale UP if: CPU > 60% OR Memory > 70%
- Scale DOWN if: Both below thresholds for 5 minutes
- Min replicas: 1 | Max replicas: 10

### Test Auto-Scaling
```bash
# Terminal 1: Watch pods
kubectl get pods -l app=ids -w

# Terminal 2: Generate load
locust -f scripts/locustfile.py -H http://localhost:8000

# Watch scaling happen!
# Low load (10 req/sec): 1 pod
# High load (500 req/sec): 2-5 pods
# Peak load (2000 req/sec): 10 pods
```

---

## ğŸ“‚ Project Structure

```
cloud-ids-project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI service (8.9 KB)
â”‚   â””â”€â”€ README.md            # API documentation
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocess_cicids2017.py  # Data pipeline (5.3 KB)
â”‚   â”œâ”€â”€ train_autoencoder.py      # Model training (5.8 KB)
â”‚   â”œâ”€â”€ evaluate_model.py         # Evaluation metrics
â”‚   â”œâ”€â”€ locustfile.py             # Load testing
â”‚   â””â”€â”€ README.md                 # Pipeline guide
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ deploy.yaml          # Kubernetes deployment
â”‚   â”œâ”€â”€ service.yaml         # Kubernetes service
â”‚   â”œâ”€â”€ hpa.yaml             # Auto-scaler config
â”‚   â””â”€â”€ README.md            # K8s guide
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ ae.pth               # PyTorch model weights
â”‚   â”œâ”€â”€ scaler.joblib        # Feature scaler
â”‚   â””â”€â”€ threshold.json       # Anomaly threshold
â”œâ”€â”€ Raw Data/
â”‚   â”œâ”€â”€ Friday-*.csv         # CIC-IDS2017 data (91-97 MB each)
â”‚   â”œâ”€â”€ Monday-*.csv         # (256 MB)
â”‚   â”œâ”€â”€ Tuesday-*.csv        # (166 MB)
â”‚   â”œâ”€â”€ Wednesday-*.csv      # (272 MB)
â”‚   â””â”€â”€ Thursday-*.csv       # (87-103 MB)
â”œâ”€â”€ Dockerfile              # Container image definition
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ model.pkl               # Trained model (773 KB)
â”œâ”€â”€ worker.py               # Legacy inference module
â”œâ”€â”€ README.md               # Main documentation
â”œâ”€â”€ .gitignore              # Git exclusions
â””â”€â”€ AUDIT_REPORT_DEC2025.md # Comprehensive audit report
```

---

## ğŸ”‘ Key Metrics & Results

### Model Performance
```
Training Data: 2,264,185 samples
Testing Data: 566,046 samples
Detected Anomalies: 57,099 (10.08%)
Detection Rate: 10.08% (matches expected 10%)
Inference Speed: <1ms per sample
Peak Throughput: 1000+ samples/sec
```

### Deployment Performance
```
Uptime: 5+ days continuous
Pod Status: 1/1 Ready
API Availability: 100%
Response Latency: <1ms (health check)
Memory Usage: 28-40% (per pod)
CPU Usage: 15-20% (baseline)
```

### Auto-Scaling Test Results
```
Baseline (10 req/sec): 1 pod at 18% CPU
Medium Load (100 req/sec): 1 pod at 45% CPU
High Load (500 req/sec): 2 pods at 35% CPU each
Peak Load (2000 req/sec): 10 pods at 25% CPU each
Scale-up Time: 15-20 seconds
Scale-down Time: 5 minutes (cooldown)
```

---

## ğŸ› Common Issues & Solutions

### Issue 1: `.venv` not found
```bash
# Solution: Create virtual environment
python -m venv .venv

# Activate it
.\.venv\Scripts\Activate.ps1  # Windows
source .venv/bin/activate      # macOS/Linux
```

### Issue 2: Feature mismatch error
```
Error: "features do not match model: expected 72, got 50"

Solution:
- Check input has exactly 72 features
- Use /model_info endpoint to see required features
- Ensure features are in correct order
```

### Issue 3: Kubernetes pod stuck in "Pending"
```bash
# Check pod status
kubectl describe pod <pod-name>

# Likely causes:
# 1. Docker image not pulled yet (give it 30 seconds)
# 2. Not enough cluster resources
# 3. Image doesn't exist on Docker Hub

# Solution:
kubectl delete pod <pod-name>  # Force restart
```

### Issue 4: Docker build takes too long
```
Expected Times:
- First build: 20-30 minutes (downloads PyTorch ~900MB)
- Subsequent builds: 2-3 minutes (layer caching)

Solution: Be patient with first build!
```

---

## ğŸ“š Team References

### Understanding the Model
- **Algorithm:** Isolation Forest (anomaly detection)
- **Input:** 72 normalized network flow features
- **Output:** Anomaly prediction (-1 = anomaly, 1 = normal)
- **Threshold:** Contamination = 0.10 (expect 10% anomalies)
- **Paper:** "Isolation Forest" by Liu et al. (2008)

### Understanding Kubernetes
- **Deployment:** Manages pod replicas
- **Service:** Exposes pods internally/externally
- **HPA:** Automatically scales replicas based on metrics
- **Pod:** Smallest deployable unit (usually 1 container)

### Understanding FastAPI
- **Framework:** Modern, fast, auto-documenting
- **Async:** Handles concurrent requests efficiently
- **Auto-docs:** Visit `/docs` for interactive API docs
- **Validation:** Type hints validate input/output

---

## ğŸ‘¥ Team Member Responsibilities

### Data Team
- Understand CIC-IDS2017 dataset structure
- Run `scripts/preprocess_cicids2017.py` for preprocessing
- Validate feature extraction and normalization
- File: `scripts/README.md`

### ML Team
- Train and evaluate models
- Run `scripts/train_autoencoder.py`
- Validate anomaly detection performance
- Monitor model metrics (`scripts/evaluate_model.py`)
- File: `scripts/README.md`

### Backend/API Team
- Maintain FastAPI service (`app/main.py`)
- Add new endpoints as needed
- Ensure API performance (<1ms latency)
- Handle validation and error responses
- File: `app/README.md`

### DevOps/Infrastructure Team
- Manage Docker image builds and pushes
- Maintain Kubernetes manifests
- Monitor deployment health
- Scale infrastructure as needed
- File: `k8s/README.md`

---

## ğŸ“ Getting Help

### File Locations for Questions
- **API Questions:** See `app/README.md`
- **ML Pipeline Questions:** See `scripts/README.md`
- **Kubernetes Questions:** See `k8s/README.md`
- **General Issues:** See `AUDIT_REPORT_DEC2025.md`
- **Development Guide:** See `.github/copilot-instructions.md`

### Quick Debugging
```bash
# Check deployment status
kubectl get all

# Check pod logs
kubectl logs -l app=ids -f

# Test API health
curl http://localhost:8000/health

# Monitor resources
kubectl top pods -l app=ids
kubectl top nodes

# Check auto-scaling
kubectl get hpa ids-hpa
```

---

## ğŸ“Š Useful Commands Cheat Sheet

```bash
# Kubernetes
kubectl get pods                      # List pods
kubectl get svc                       # List services
kubectl get hpa                       # List auto-scalers
kubectl describe pod <name>           # Pod details
kubectl logs <pod-name>               # View logs
kubectl port-forward svc/ids-svc 8000:80  # Local access

# Docker
docker build -t ids:latest .          # Build image
docker run -p 8000:8000 ids:latest    # Run locally
docker push siddartha6174/ids:latest  # Push to registry
docker images                         # List images
docker ps                             # Running containers

# Python/Development
python -m venv .venv                  # Create venv
pip install -r requirements.txt       # Install deps
python -m uvicorn app.main:app        # Run API locally
pytest                                # Run tests

# Git
git status                            # Check status
git log --oneline                     # View commits
git push origin main                  # Push changes
```

---

## ğŸ“ Learning Resources

### For Anomaly Detection
- CIC-IDS2017 Dataset: https://www.unb.ca/cic/datasets/ids-2017.html
- Isolation Forest Paper: https://cs.nju.edu.cn/iip/pdf/ICDM08.pdf

### For Machine Learning
- scikit-learn: https://scikit-learn.org/
- PyTorch: https://pytorch.org/

### For APIs & Deployment
- FastAPI: https://fastapi.tiangolo.com/
- Kubernetes: https://kubernetes.io/docs/
- Docker: https://docs.docker.com/

---

## âœ… Project Checklist for Teams

- [ ] **Data Team:** Understand dataset structure and preprocessing
- [ ] **ML Team:** Train and validate anomaly detection model
- [ ] **API Team:** Test all 5 endpoints locally
- [ ] **DevOps Team:** Deploy to Kubernetes and verify
- [ ] **Everyone:** Review DEMO_PRESENTATION.md for full context
- [ ] **Everyone:** Set up local development environment
- [ ] **Everyone:** Understand auto-scaling behavior

---

## ğŸ“ Final Notes

### Project Status
- âœ… Production Ready
- âœ… 5+ days uptime
- âœ… All endpoints working
- âœ… Auto-scaling tested and verified
- âœ… Documentation complete

### Next Steps
1. Review this README thoroughly
2. Set up local development environment
3. Test API endpoints locally
4. Deploy to Kubernetes
5. Monitor auto-scaling behavior

### Contact & Support
For questions about specific components, refer to the relevant README file in that directory.

---

**Last Updated:** December 6, 2025  
**Maintainer:** Your Team  
**Status:** âœ… Production Ready

